[34m[1mwandb[0m: Detected [openai] in use.
[34m[1mwandb[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[34m[1mwandb[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/
[36m(pid=1848768)[0m /ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(pid=1848768)[0m No module named 'vllm._version'
[36m(pid=1848768)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(pid=1848949)[0m /ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(pid=1848949)[0m No module named 'vllm._version'
[36m(pid=1848949)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(WorkerDict pid=1848768)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=1848768)[0m   "_name_or_path": "models/Qwen/Qwen2.5-3B-Instruct",
[36m(WorkerDict pid=1848768)[0m   "architectures": [
[36m(WorkerDict pid=1848768)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=1848768)[0m   ],
[36m(WorkerDict pid=1848768)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=1848768)[0m   "eos_token_id": 151645,
[36m(WorkerDict pid=1848768)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=1848768)[0m   "hidden_size": 2048,
[36m(WorkerDict pid=1848768)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=1848768)[0m   "intermediate_size": 11008,
[36m(WorkerDict pid=1848768)[0m   "max_position_embeddings": 32768,
[36m(WorkerDict pid=1848768)[0m   "max_window_layers": 70,
[36m(WorkerDict pid=1848768)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=1848768)[0m   "num_attention_heads": 16,
[36m(WorkerDict pid=1848768)[0m   "num_hidden_layers": 36,
[36m(WorkerDict pid=1848768)[0m   "num_key_value_heads": 2,
[36m(WorkerDict pid=1848768)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=1848768)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=1848768)[0m   "rope_scaling": null,
[36m(WorkerDict pid=1848768)[0m   "rope_theta": 1000000.0,
[36m(WorkerDict pid=1848768)[0m   "sliding_window": null,
[36m(WorkerDict pid=1848768)[0m   "tie_word_embeddings": true,
[36m(WorkerDict pid=1848768)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=1848768)[0m   "transformers_version": "4.47.1",
[36m(WorkerDict pid=1848768)[0m   "use_cache": true,
[36m(WorkerDict pid=1848768)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=1848768)[0m   "vocab_size": 151936
[36m(WorkerDict pid=1848768)[0m }
[36m(WorkerDict pid=1848768)[0m
[36m(WorkerDict pid=1848768)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  3.72it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  4.42it/s]
[36m(WorkerDict pid=1848768)[0m NCCL version 2.20.5+cuda12.4
[36m(WorkerDict pid=1848768)[0m Qwen2ForCausalLM contains 3.09B parameters
[36m(WorkerDict pid=1848768)[0m wrap_policy: functools.partial(<function _or_policy at 0x779556df4af0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x779556df49d0>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
[36m(WorkerDict pid=1848768)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=1848949)[0m wrap_policy: functools.partial(<function _or_policy at 0x7e0026874af0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7e00268749d0>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
[36m(WorkerDict pid=1848768)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[36m(WorkerDict pid=1848949)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.19it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  3.62it/s]
[36m(WorkerDict pid=1848768)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=1848768)[0m   "_name_or_path": "models/Qwen/Qwen2.5-3B-Instruct",
[36m(WorkerDict pid=1848768)[0m   "architectures": [
[36m(WorkerDict pid=1848768)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=1848768)[0m   ],
[36m(WorkerDict pid=1848768)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=1848768)[0m   "eos_token_id": 151645,
[36m(WorkerDict pid=1848768)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=1848768)[0m   "hidden_size": 2048,
[36m(WorkerDict pid=1848768)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=1848768)[0m   "intermediate_size": 11008,
[36m(WorkerDict pid=1848768)[0m   "max_position_embeddings": 32768,
[36m(WorkerDict pid=1848768)[0m   "max_window_layers": 70,
[36m(WorkerDict pid=1848768)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=1848768)[0m   "num_attention_heads": 16,
[36m(WorkerDict pid=1848768)[0m   "num_hidden_layers": 36,
[36m(WorkerDict pid=1848768)[0m   "num_key_value_heads": 2,
[36m(WorkerDict pid=1848768)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=1848768)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=1848768)[0m   "rope_scaling": null,
[36m(WorkerDict pid=1848768)[0m   "rope_theta": 1000000.0,
[36m(WorkerDict pid=1848768)[0m   "sliding_window": null,
[36m(WorkerDict pid=1848768)[0m   "tie_word_embeddings": true,
[36m(WorkerDict pid=1848768)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=1848768)[0m   "transformers_version": "4.47.1",
[36m(WorkerDict pid=1848768)[0m   "use_cache": true,
[36m(WorkerDict pid=1848768)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=1848768)[0m   "vocab_size": 151936
[36m(WorkerDict pid=1848768)[0m }
[36m(WorkerDict pid=1848768)[0m
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:06<00:06,  6.00s/it]
[36m(WorkerDict pid=1848949)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:08<00:00,  4.46s/it]
[36m(WorkerDict pid=1848768)[0m Qwen2ForCausalLM contains 3.09B parameters
[36m(WorkerDict pid=1848768)[0m wrap_policy: functools.partial(<function _or_policy at 0x779556df4af0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x779556df49d0>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
[36m(WorkerDict pid=1848949)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=1848768)[0m Total steps: 203, num_warmup_steps: 192
[36m(WorkerDict pid=1848768)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=1848768)[0m Before building vllm rollout, memory allocated (GB): 5.783238887786865, memory reserved (GB): 5.78515625
[36m(WorkerDict pid=1848768)[0m WARNING 10-16 11:30:19 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(WorkerDict pid=1848949)[0m wrap_policy: functools.partial(<function _or_policy at 0x7e0026874af0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7e00268749d0>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
[36m(WorkerDict pid=1848949)[0m Total steps: 203, num_warmup_steps: 192
[36m(WorkerDict pid=1848949)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=1848768)[0m /ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
[36m(WorkerDict pid=1848768)[0m   @torch.library.impl_abstract("xformers_flash::flash_fwd")
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:06<00:06,  6.19s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:08<00:00,  4.48s/it]
[36m(WorkerDict pid=1848768)[0m local rank 0
[36m(WorkerDict pid=1848768)[0m INFO 10-16 11:30:19 selector.py:115] Using XFormers backend.
[36m(WorkerDict pid=1848768)[0m   @torch.library.impl_abstract("xformers_flash::flash_bwd")
[36m(WorkerDict pid=1848949)[0m NCCL version 2.20.5+cuda12.4
[36m(WorkerDict pid=1848768)[0m before init cache memory allocated: 12.436768256GB, reserved: 12.524191744GB
[36m(WorkerDict pid=1848768)[0m after init cache memory allocated: 18.703058432GB, reserved: 18.79048192GB
[36m(WorkerDict pid=1848768)[0m kwargs: {'n': 1, 'logprobs': 1, 'max_tokens': 500, 'detokenize': False, 'temperature': 1, 'top_k': -1, 'top_p': 0.95, 'ignore_eos': False}
[36m(WorkerDict pid=1848768)[0m After building vllm rollout, memory allocated (GB): 11.63492727279663, memory reserved (GB): 17.5
[36m(WorkerDict pid=1848949)[0m WARNING 10-16 11:30:19 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(WorkerDict pid=1848949)[0m local rank 0
[36m(WorkerDict pid=1848949)[0m INFO 10-16 11:30:20 selector.py:115] Using XFormers backend.[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(WorkerDict pid=1848768)[0m /ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=1848768)[0m   warnings.warn(
[36m(WorkerDict pid=1848949)[0m /ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=1848949)[0m   @torch.library.impl_abstract("xformers_flash::flash_fwd")
[36m(WorkerDict pid=1848949)[0m   @torch.library.impl_abstract("xformers_flash::flash_bwd")
[36m(WorkerDict pid=1848768)[0m After building sharding manager, memory allocated (GB): 11.63492727279663, memory reserved (GB): 17.5
Training:   0%|                                                                                                                                                                | 0/203 [00:09<?, ?it/s]
epoch 0, step 1
Error executing job with overrides: ['data.train_files=data/CriticSearch_dataset/train.parquet', 'data.val_files=data/CriticSearch_dataset/test.parquet', 'data.train_data_num=null', 'data.val_data_num=null', 'data.train_batch_size=64', 'data.val_batch_size=64', 'data.max_prompt_length=4096', 'data.max_response_length=500', 'data.max_start_length=2048', 'data.max_obs_length=2048', 'data.shuffle_train_dataloader=True', 'algorithm.adv_estimator=grpo', 'actor_rollout_ref.model.path=models/Qwen/Qwen2.5-3B-Instruct', 'actor_rollout_ref.model.enable_gradient_checkpointing=true', 'actor_rollout_ref.model.use_remove_padding=True', 'actor_rollout_ref.actor.optim.lr=1e-6', 'actor_rollout_ref.actor.optim.lr_warmup_steps_ratio=0.95', 'actor_rollout_ref.actor.use_kl_loss=true', 'actor_rollout_ref.actor.ppo_mini_batch_size=256', 'actor_rollout_ref.actor.ppo_micro_batch_size=64', 'actor_rollout_ref.actor.fsdp_config.param_offload=true', 'actor_rollout_ref.actor.fsdp_config.grad_offload=true', 'actor_rollout_ref.actor.fsdp_config.optimizer_offload=true', 'actor_rollout_ref.rollout.log_prob_micro_batch_size=128', 'actor_rollout_ref.rollout.tensor_model_parallel_size=1', 'actor_rollout_ref.rollout.name=vllm', 'actor_rollout_ref.rollout.gpu_memory_utilization=0.6', 'actor_rollout_ref.ref.log_prob_micro_batch_size=128', 'actor_rollout_ref.ref.fsdp_config.param_offload=True', 'actor_rollout_ref.actor.kl_loss_coef=0.001', 'actor_rollout_ref.actor.kl_loss_type=low_var_kl', 'algorithm.no_think_rl=false', 'actor_rollout_ref.rollout.n_agent=5', 'actor_rollout_ref.rollout.temperature=1', 'actor_rollout_ref.actor.state_masking=True', 'trainer.logger=[wandb]', 'trainer.val_only=false', 'trainer.val_before_train=False', 'trainer.default_hdfs_dir=null', 'trainer.n_gpus_per_node=2', 'trainer.nnodes=1', 'trainer.save_freq=50', 'trainer.test_freq=300', 'trainer.project_name=ZeroSearch', 'trainer.experiment_name=Qwen2.5-3B-Instruct_GRPO_wiki_models/iic/Simulation_LLM_google_3B_0_0.5_wiki_turns_5', 'trainer.total_epochs=10', 'trainer.total_training_steps=203', 'trainer.default_hdfs_dir=null', 'trainer.default_local_dir=verl_checkpoints/Qwen2.5-3B-Instruct_GRPO_wiki_models/iic/Simulation_LLM_google_3B_0_0.5_wiki_turns_5', 'trainer.max_turns=5', 'trainer.reward_function=f1', 'trainer.do_search=True', 'retriever.start_threshold=0', 'retriever.end_threshold=0.5', 'retriever.llm_ip=localhost', 'retriever.search_mode=wiki', 'retriever.search_engine=wiki', 'retriever.topk=5', 'retriever.simulate_llm=models/iic/Simulation_LLM_google_3B']
Traceback (most recent call last):
  File "/ssd/chenxi/research/CriticSearch/verl/trainer/main_ppo.py", line 156, in main
    main_task(config)
  File "/ssd/chenxi/research/CriticSearch/verl/trainer/main_ppo.py", line 229, in main_task
    trainer.fit()
  File "/ssd/chenxi/research/CriticSearch/verl/trainer/ppo/ray_trainer.py", line 815, in fit
    final_gen_batch_output, trajectory_turns = user_generation_manager.run_llm_loop(
  File "/ssd/chenxi/research/CriticSearch/llm_agent/user_generation.py", line 417, in run_llm_loop
    gen_output = self._generate_with_gpu_padding(rollings_active)
  File "/ssd/chenxi/research/CriticSearch/llm_agent/user_generation.py", line 356, in _generate_with_gpu_padding
    return self.actor_rollout_wg.generate_sequences(active_batch)
  File "/ssd/chenxi/research/CriticSearch/verl/single_controller/ray/base.py", line 42, in func
    output = ray.get(output)
  File "/ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 22, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 104, in wrapper
    return func(*args, **kwargs)
  File "/ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/ray/_private/worker.py", line 2962, in get
    values, debugger_breakpoint = worker.get_objects(
  File "/ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/ray/_private/worker.py", line 1026, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(OutOfMemoryError): [36mray::WorkerDict.actor_rollout_generate_sequences()[39m (pid=1848768, ip=10.10.37.164, actor_id=6329f1a520115022f31738b101000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x77952c0d6c70>)
  File "/ssd/chenxi/research/CriticSearch/verl/single_controller/ray/base.py", line 399, in func
    return getattr(self.worker_dict[key], name)(*args, **kwargs)
  File "/ssd/chenxi/research/CriticSearch/verl/single_controller/base/decorator.py", line 404, in inner
    return func(*args, **kwargs)
  File "/ssd/chenxi/research/CriticSearch/verl/workers/fsdp_workers.py", line 446, in generate_sequences
    with self.rollout_sharding_manager:
  File "/ssd/chenxi/research/CriticSearch/verl/workers/sharding_manager/fsdp_vllm.py", line 75, in __enter__
    self.inference_engine.sync_model_weights(params, load_format=load_format)
  File "/ssd/chenxi/research/CriticSearch/verl/third_party/vllm/vllm_v_0_6_3/llm.py", line 197, in sync_model_weights
    self.llm_engine.sync_model_weights(actor_weights=actor_weights, load_format=load_format)
  File "/ssd/chenxi/research/CriticSearch/verl/third_party/vllm/vllm_v_0_6_3/llm_engine_sp.py", line 405, in sync_model_weights
    self.model_executor.sync_model_weights(actor_weights=actor_weights, load_format=load_format)
  File "/ssd/chenxi/research/CriticSearch/verl/third_party/vllm/vllm_v_0_6_3/spmd_gpu_executor.py", line 213, in sync_model_weights
    self.worker.sync_model_weights(actor_weights=actor_weights, load_format=load_format)
  File "/ssd/chenxi/research/CriticSearch/verl/third_party/vllm/vllm_v_0_6_3/worker.py", line 281, in sync_model_weights
    load_dtensor_weights(actor_weights, self.model_runner.model)
  File "/ssd/chenxi/research/CriticSearch/verl/third_party/vllm/vllm_v_0_6_3/dtensor_weight_loaders.py", line 368, in load_dtensor_weights
    vllm_model = vllm_model.cuda()
  File "/ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 916, in cuda
    return self._apply(lambda t: t.cuda(device))
  File "/ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 916, in <lambda>
    return self._apply(lambda t: t.cuda(device))
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 23.52 GiB of which 65.69 MiB is free. Including non-PyTorch memory, this process has 23.43 GiB memory in use. Of the allocated memory 22.68 GiB is allocated by PyTorch, and 108.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
