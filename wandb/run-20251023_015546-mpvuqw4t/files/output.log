[34m[1mwandb[0m: Detected [openai] in use.
[34m[1mwandb[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[34m[1mwandb[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/
[36m(pid=2103537)[0m /ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(pid=2103537)[0m No module named 'vllm._version'
[36m(pid=2103537)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(pid=2103873)[0m /ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(pid=2103873)[0m No module named 'vllm._version'
[36m(pid=2103873)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(WorkerDict pid=2103537)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=2103537)[0m   "_name_or_path": "models/Qwen/Qwen2.5-0.5B-Instruct",
[36m(WorkerDict pid=2103537)[0m   "architectures": [
[36m(WorkerDict pid=2103537)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=2103537)[0m   ],
[36m(WorkerDict pid=2103537)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=2103537)[0m   "eos_token_id": 151645,
[36m(WorkerDict pid=2103537)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=2103537)[0m   "hidden_size": 896,
[36m(WorkerDict pid=2103537)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=2103537)[0m   "intermediate_size": 4864,
[36m(WorkerDict pid=2103537)[0m   "max_position_embeddings": 32768,
[36m(WorkerDict pid=2103537)[0m   "max_window_layers": 21,
[36m(WorkerDict pid=2103537)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=2103537)[0m   "num_attention_heads": 14,
[36m(WorkerDict pid=2103537)[0m   "num_hidden_layers": 24,
[36m(WorkerDict pid=2103537)[0m   "num_key_value_heads": 2,
[36m(WorkerDict pid=2103537)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=2103537)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=2103537)[0m   "rope_scaling": null,
[36m(WorkerDict pid=2103537)[0m   "rope_theta": 1000000.0,
[36m(WorkerDict pid=2103537)[0m   "sliding_window": null,
[36m(WorkerDict pid=2103537)[0m   "tie_word_embeddings": true,
[36m(WorkerDict pid=2103537)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=2103537)[0m   "transformers_version": "4.47.1",
[36m(WorkerDict pid=2103537)[0m   "use_cache": true,
[36m(WorkerDict pid=2103537)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=2103537)[0m   "vocab_size": 151936
[36m(WorkerDict pid=2103537)[0m }
[36m(WorkerDict pid=2103537)[0m
[36m(WorkerDict pid=2103537)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(WorkerDict pid=2103537)[0m NCCL version 2.20.5+cuda12.4
[36m(WorkerDict pid=2103537)[0m Qwen2ForCausalLM contains 494.03M parameters
[36m(WorkerDict pid=2103537)[0m wrap_policy: functools.partial(<function _or_policy at 0x7894bcb3c5e0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7894bcb3c4c0>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
[36m(WorkerDict pid=2103537)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=2103873)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[36m(WorkerDict pid=2103537)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=2103537)[0m   "_name_or_path": "models/Qwen/Qwen2.5-0.5B-Instruct",
[36m(WorkerDict pid=2103537)[0m   "architectures": [
[36m(WorkerDict pid=2103537)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=2103537)[0m   ],
[36m(WorkerDict pid=2103537)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=2103537)[0m   "eos_token_id": 151645,
[36m(WorkerDict pid=2103537)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=2103537)[0m   "hidden_size": 896,
[36m(WorkerDict pid=2103537)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=2103537)[0m   "intermediate_size": 4864,
[36m(WorkerDict pid=2103537)[0m   "max_position_embeddings": 32768,
[36m(WorkerDict pid=2103537)[0m   "max_window_layers": 21,
[36m(WorkerDict pid=2103537)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=2103537)[0m   "num_attention_heads": 14,
[36m(WorkerDict pid=2103537)[0m   "num_hidden_layers": 24,
[36m(WorkerDict pid=2103537)[0m   "num_key_value_heads": 2,
[36m(WorkerDict pid=2103537)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=2103537)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=2103537)[0m   "rope_scaling": null,
[36m(WorkerDict pid=2103537)[0m   "rope_theta": 1000000.0,
[36m(WorkerDict pid=2103537)[0m   "sliding_window": null,
[36m(WorkerDict pid=2103537)[0m   "tie_word_embeddings": true,
[36m(WorkerDict pid=2103537)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=2103537)[0m   "transformers_version": "4.47.1",
[36m(WorkerDict pid=2103537)[0m   "use_cache": true,
[36m(WorkerDict pid=2103537)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=2103537)[0m   "vocab_size": 151936
[36m(WorkerDict pid=2103537)[0m }
[36m(WorkerDict pid=2103537)[0m
[36m(WorkerDict pid=2103537)[0m Qwen2ForCausalLM contains 494.03M parameters
[36m(WorkerDict pid=2103537)[0m Total steps: 203, num_warmup_steps: 192
[36m(WorkerDict pid=2103537)[0m Before building vllm rollout, memory allocated (GB): 0.9205718040466309, memory reserved (GB): 0.958984375
[36m(WorkerDict pid=2103537)[0m WARNING 10-23 01:56:09 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(WorkerDict pid=2103873)[0m wrap_policy: functools.partial(<function _or_policy at 0x7eb3f11bc5e0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7eb3f11bc4c0>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(WorkerDict pid=2103873)[0m Actor use_remove_padding=True[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2103537)[0m local rank 0
[36m(WorkerDict pid=2103537)[0m INFO 10-23 01:56:09 selector.py:115] Using XFormers backend.
[36m(WorkerDict pid=2103537)[0m /ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
[36m(WorkerDict pid=2103537)[0m   @torch.library.impl_abstract("xformers_flash::flash_fwd")
[36m(WorkerDict pid=2103873)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(WorkerDict pid=2103537)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[36m(WorkerDict pid=2103537)[0m /ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
[36m(WorkerDict pid=2103537)[0m   @torch.library.impl_abstract("xformers_flash::flash_bwd")
[36m(WorkerDict pid=2103873)[0m Total steps: 203, num_warmup_steps: 192
[36m(WorkerDict pid=2103873)[0m NCCL version 2.20.5+cuda12.4
[36m(WorkerDict pid=2103537)[0m before init cache memory allocated: 1.99052032GB, reserved: 2.057306112GB
[36m(WorkerDict pid=2103537)[0m after init cache memory allocated: 14.57343232GB, reserved: 14.640218112GB
[36m(WorkerDict pid=2103873)[0m WARNING 10-23 01:56:09 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(WorkerDict pid=2103873)[0m local rank 0
[36m(WorkerDict pid=2103873)[0m INFO 10-23 01:56:10 selector.py:115] Using XFormers backend.[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2103873)[0m kwargs: {'n': 1, 'logprobs': 1, 'max_tokens': 500, 'detokenize': False, 'temperature': 1, 'top_k': -1, 'top_p': 0.95, 'ignore_eos': False}
[36m(WorkerDict pid=2103873)[0m /ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=2103873)[0m   warnings.warn(
[36m(WorkerDict pid=2103873)[0m /ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.[32m [repeated 2x across cluster][0m
[36m(WorkerDict pid=2103873)[0m   @torch.library.impl_abstract("xformers_flash::flash_fwd")
[36m(WorkerDict pid=2103873)[0m   @torch.library.impl_abstract("xformers_flash::flash_bwd")
Training:   0%|                                                                                                                                                             | 0/203 [00:00<?, ?it/s]
[36m(WorkerDict pid=2103537)[0m After building vllm rollout, memory allocated (GB): 12.651166439056396, memory reserved (GB): 13.634765625
[36m(WorkerDict pid=2103537)[0m After building sharding manager, memory allocated (GB): 12.651166439056396, memory reserved (GB): 13.634765625
epoch 0, step 1
[2025-10-23 01:56:50,205][httpx][INFO] - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 200 OK"
[2025-10-23 01:56:50,217][httpx][INFO] - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 200 OK"
[2025-10-23 01:56:50,218][httpx][INFO] - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 200 OK"
[2025-10-23 01:56:50,219][httpx][INFO] - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 200 OK"
[2025-10-23 01:56:50,229][httpx][INFO] - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 200 OK"
[2025-10-23 01:56:50,230][httpx][INFO] - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 200 OK"
[2025-10-23 01:56:50,295][httpx][INFO] - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 200 OK"
[2025-10-23 01:56:50,646][httpx][INFO] - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 200 OK"
[2025-10-23 01:56:50,695][httpx][INFO] - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 200 OK"
[2025-10-23 01:56:50,697][httpx][INFO] - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 200 OK"
[2025-10-23 01:56:50,734][httpx][INFO] - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 200 OK"
[2025-10-23 01:56:50,747][httpx][INFO] - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 200 OK"
[2025-10-23 01:56:50,783][httpx][INFO] - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 200 OK"
[2025-10-23 01:56:50,869][httpx][INFO] - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 200 OK"
[2025-10-23 01:56:50,940][httpx][INFO] - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 200 OK"
[2025-10-23 01:56:51,073][httpx][INFO] - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 200 OK"
[2025-10-23 01:56:51,118][httpx][INFO] - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 200 OK"
[2025-10-23 01:56:51,141][httpx][INFO] - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 200 OK"
[2025-10-23 01:56:51,194][httpx][INFO] - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 200 OK"
[2025-10-23 01:56:51,205][httpx][INFO] - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 200 OK"
[2025-10-23 01:56:51,242][httpx][INFO] - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 200 OK"
[2025-10-23 01:56:51,303][httpx][INFO] - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 200 OK"
[2025-10-23 01:56:51,438][httpx][INFO] - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 200 OK"
[2025-10-23 01:56:51,463][httpx][INFO] - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 200 OK"
[2025-10-23 01:56:51,483][httpx][INFO] - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 200 OK"
[2025-10-23 01:56:51,559][httpx][INFO] - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 200 OK"
[2025-10-23 01:56:51,583][httpx][INFO] - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 200 OK"
[2025-10-23 01:56:51,623][httpx][INFO] - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 200 OK"
[2025-10-23 01:56:51,677][httpx][INFO] - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 200 OK"
[2025-10-23 01:56:51,834][httpx][INFO] - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 200 OK"
[2025-10-23 01:56:51,843][httpx][INFO] - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 200 OK"
[2025-10-23 01:56:51,848][httpx][INFO] - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 200 OK"
[2025-10-23 01:56:52,146][httpx][INFO] - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 200 OK"
[2025-10-23 01:56:52,290][httpx][INFO] - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 200 OK"
[2025-10-23 01:56:52,296][httpx][INFO] - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 200 OK"
[2025-10-23 01:56:52,330][httpx][INFO] - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 200 OK"
[2025-10-23 01:56:52,574][httpx][INFO] - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 200 OK"
[2025-10-23 01:56:52,695][httpx][INFO] - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 200 OK"
[2025-10-23 01:56:52,727][httpx][INFO] - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 200 OK"
[2025-10-23 01:56:52,818][httpx][INFO] - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 200 OK"
