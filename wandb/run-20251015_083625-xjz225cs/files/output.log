[34m[1mwandb[0m: Detected [openai] in use.
[34m[1mwandb[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[34m[1mwandb[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/
[36m(pid=1456387)[0m /ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(pid=1456387)[0m No module named 'vllm._version'
[36m(pid=1456387)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(pid=1456574)[0m /ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(pid=1456574)[0m No module named 'vllm._version'
[36m(pid=1456574)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(pid=1456575)[0m /ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(pid=1456575)[0m No module named 'vllm._version'
[36m(pid=1456575)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(WorkerDict pid=1456387)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=1456387)[0m   "_name_or_path": "models/iic/ZeroSearch_google_V2_Qwen2___5_3B_Instruct",
[36m(WorkerDict pid=1456387)[0m   "architectures": [
[36m(WorkerDict pid=1456387)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=1456387)[0m   ],
[36m(WorkerDict pid=1456387)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=1456387)[0m   "eos_token_id": 151645,
[36m(WorkerDict pid=1456387)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=1456387)[0m   "hidden_size": 2048,
[36m(WorkerDict pid=1456387)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=1456387)[0m   "intermediate_size": 11008,
[36m(WorkerDict pid=1456387)[0m   "max_position_embeddings": 32768,
[36m(WorkerDict pid=1456387)[0m   "max_window_layers": 70,
[36m(WorkerDict pid=1456387)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=1456387)[0m   "num_attention_heads": 16,
[36m(WorkerDict pid=1456387)[0m   "num_hidden_layers": 36,
[36m(WorkerDict pid=1456387)[0m   "num_key_value_heads": 2,
[36m(WorkerDict pid=1456387)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=1456387)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=1456387)[0m   "rope_scaling": null,
[36m(WorkerDict pid=1456387)[0m   "rope_theta": 1000000.0,
[36m(WorkerDict pid=1456387)[0m   "sliding_window": null,
[36m(WorkerDict pid=1456387)[0m   "tie_word_embeddings": true,
[36m(WorkerDict pid=1456387)[0m   "torch_dtype": "float32",
[36m(WorkerDict pid=1456387)[0m   "transformers_version": "4.47.1",
[36m(WorkerDict pid=1456387)[0m   "use_cache": true,
[36m(WorkerDict pid=1456387)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=1456387)[0m   "vocab_size": 151936
[36m(WorkerDict pid=1456387)[0m }
[36m(WorkerDict pid=1456387)[0m
[36m(WorkerDict pid=1456387)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [01:23<02:47, 83.75s/it]
[36m(pid=1456573)[0m /ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(pid=1456573)[0m No module named 'vllm._version'
[36m(pid=1456573)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(WorkerDict pid=1456573)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s][32m [repeated 3x across cluster][0m
Loading checkpoint shards:  67%|██████▋   | 2/3 [02:42<01:21, 81.10s/it][32m [repeated 4x across cluster][0m
Loading checkpoint shards: 100%|██████████| 3/3 [03:36<00:00, 72.21s/it]
Loading checkpoint shards:  67%|██████▋   | 2/3 [02:43<01:21, 81.14s/it][32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=1456387)[0m NCCL version 2.20.5+cuda12.4
[36m(WorkerDict pid=1456387)[0m Qwen2ForCausalLM contains 3.09B parameters
[36m(WorkerDict pid=1456387)[0m wrap_policy: functools.partial(<function _or_policy at 0x74daf9233b80>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x74daf9233a60>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
[36m(WorkerDict pid=1456387)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=1456387)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=1456387)[0m   "_name_or_path": "models/iic/ZeroSearch_google_V2_Qwen2___5_3B_Instruct",
[36m(WorkerDict pid=1456387)[0m   "architectures": [
[36m(WorkerDict pid=1456387)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=1456387)[0m   ],
[36m(WorkerDict pid=1456387)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=1456387)[0m   "eos_token_id": 151645,
[36m(WorkerDict pid=1456387)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=1456387)[0m   "hidden_size": 2048,
[36m(WorkerDict pid=1456387)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=1456387)[0m   "intermediate_size": 11008,
[36m(WorkerDict pid=1456387)[0m   "max_position_embeddings": 32768,
[36m(WorkerDict pid=1456387)[0m   "max_window_layers": 70,
[36m(WorkerDict pid=1456387)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=1456387)[0m   "num_attention_heads": 16,
[36m(WorkerDict pid=1456387)[0m   "num_hidden_layers": 36,
[36m(WorkerDict pid=1456387)[0m   "num_key_value_heads": 2,
[36m(WorkerDict pid=1456387)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=1456387)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=1456387)[0m   "rope_scaling": null,
[36m(WorkerDict pid=1456387)[0m   "rope_theta": 1000000.0,
[36m(WorkerDict pid=1456387)[0m   "sliding_window": null,
[36m(WorkerDict pid=1456387)[0m   "tie_word_embeddings": true,
[36m(WorkerDict pid=1456387)[0m   "torch_dtype": "float32",
[36m(WorkerDict pid=1456387)[0m   "transformers_version": "4.47.1",
[36m(WorkerDict pid=1456387)[0m   "use_cache": true,
[36m(WorkerDict pid=1456387)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=1456387)[0m   "vocab_size": 151936
[36m(WorkerDict pid=1456387)[0m }
[36m(WorkerDict pid=1456387)[0m
[36m(WorkerDict pid=1456575)[0m wrap_policy: functools.partial(<function _or_policy at 0x7c2f119f3b80>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7c2f119f3a60>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=1456387)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
Loading checkpoint shards: 100%|██████████| 3/3 [03:36<00:00, 72.22s/it][32m [repeated 3x across cluster][0m
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  4.63it/s]
[36m(WorkerDict pid=1456387)[0m Qwen2ForCausalLM contains 3.09B parameters
[36m(WorkerDict pid=1456387)[0m Total steps: 203, num_warmup_steps: 192
[36m(WorkerDict pid=1456575)[0m Actor use_remove_padding=True[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=1456575)[0m wrap_policy: functools.partial(<function _or_policy at 0x7c2f119f3b80>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7c2f119f3a60>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])[32m [repeated 4x across cluster][0m
[36m(WorkerDict pid=1456387)[0m Before building vllm rollout, memory allocated (GB): 2.891395092010498, memory reserved (GB): 2.89453125
[36m(WorkerDict pid=1456387)[0m INFO 10-15 08:40:35 config.py:1670] Downcasting torch.float32 to torch.bfloat16.
[36m(WorkerDict pid=1456387)[0m WARNING 10-15 08:40:47 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(WorkerDict pid=1456575)[0m Total steps: 203, num_warmup_steps: 192[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=1456575)[0m Actor use_remove_padding=True[32m [repeated 4x across cluster][0m
[36m(WorkerDict pid=1456575)[0m INFO 10-15 08:40:35 config.py:1670] Downcasting torch.float32 to torch.bfloat16.[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=1456387)[0m local rank 0
[36m(WorkerDict pid=1456387)[0m INFO 10-15 08:40:47 selector.py:115] Using XFormers backend.
[36m(WorkerDict pid=1456387)[0m /ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
[36m(WorkerDict pid=1456387)[0m   @torch.library.impl_abstract("xformers_flash::flash_fwd")
[36m(WorkerDict pid=1456387)[0m /ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
[36m(WorkerDict pid=1456387)[0m   @torch.library.impl_abstract("xformers_flash::flash_bwd")
[36m(WorkerDict pid=1456573)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`[32m [repeated 3x across cluster][0m
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  3.75it/s][32m [repeated 4x across cluster][0m
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s][32m [repeated 3x across cluster][0m
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  3.66it/s][32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=1456573)[0m NCCL version 2.20.5+cuda12.4
[36m(WorkerDict pid=1456575)[0m WARNING 10-15 08:40:47 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=1456575)[0m local rank 0[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=1456575)[0m INFO 10-15 08:40:49 selector.py:115] Using XFormers backend.[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=1456387)[0m before init cache memory allocated: 9.331674624GB, reserved: 9.420406784GB
[36m(WorkerDict pid=1456387)[0m after init cache memory allocated: 17.4854016GB, reserved: 17.57413376GB
[36m(WorkerDict pid=1456387)[0m kwargs: {'n': 1, 'logprobs': 1, 'max_tokens': 500, 'detokenize': False, 'temperature': 1, 'top_k': -1, 'top_p': 0.95, 'ignore_eos': False}
[36m(WorkerDict pid=1456387)[0m After building vllm rollout, memory allocated (GB): 10.500895977020264, memory reserved (GB): 16.3671875
[36m(WorkerDict pid=1456387)[0m /ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=1456387)[0m   warnings.warn(
[36m(WorkerDict pid=1456573)[0m /ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.[32m [repeated 6x across cluster][0m
[36m(WorkerDict pid=1456573)[0m   @torch.library.impl_abstract("xformers_flash::flash_fwd")[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=1456573)[0m   @torch.library.impl_abstract("xformers_flash::flash_bwd")[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=1456387)[0m After building sharding manager, memory allocated (GB): 10.500895977020264, memory reserved (GB): 16.3671875
Testing:   0%|                                                                                                                                                                    | 0/781 [00:00<?, ?it/s]
ACTIVE_TRAJ_NUM: [4, 4, 4, 0]
Interaction Turns Statistics:
Finish at the 1-th turn: 0
Finish at the 2-th turn: 0
Finish at the 3-th turn: 4
Finish at the 4-th turn: 0
Finish at the 5-th turn: 0
Finish at the 6-th turn: 0
<|im_start|>system
You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>
<|im_start|>user
Answer the given question. You must conduct reasoning inside <think> and </think> first every time you get new information. After reasoning, if you find you lack some knowledge, you can call a search engine by <search> query </search> and it will return the top searched results between <information> and </information>. You can search as many times as your want. If you find no further external knowledge needed, you can directly provide the answer inside <answer> and </answer>, without detailed illustrations. For example, <answer> Beijing </answer>. Question: who got the first nobel prize in physics?
<|im_end|>
<|im_start|>assistant
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think
My previous action is invalid. If I want to search, I should put the query between <search> and </search>. If I want to give the final answer, I should put the answer between <answer> and </answer>. Let me try again.
</think>
<search>who got the first nobel prize in physics</search>

<information>Doc 1: "Nobel Prize in Physics"
Nobel Prize in Physics The Nobel Prize in Physics () is a yearly award given by the Royal Swedish Academy of Sciences for those who have made the most outstanding contributions for mankind in the field of physics. It is one of the five Nobel Prizes established by the will of Alfred Nobel in 1895 and awarded since 1901; the others being the Nobel Prize in Chemistry, Nobel Prize in Literature, Nobel Peace Prize, and Nobel Prize in Physiology or Medicine. The first Nobel Prize in Physics was awarded to physicist Wilhelm Röntgen in recognition of the extraordinary services he
Doc 2: "Nobel Prize in Physics"
receive a diploma, a medal and a document confirming the prize amount. Nobel Prize in Physics The Nobel Prize in Physics () is a yearly award given by the Royal Swedish Academy of Sciences for those who have made the most outstanding contributions for mankind in the field of physics. It is one of the five Nobel Prizes established by the will of Alfred Nobel in 1895 and awarded since 1901; the others being the Nobel Prize in Chemistry, Nobel Prize in Literature, Nobel Peace Prize, and Nobel Prize in Physiology or Medicine. The first Nobel Prize in Physics was
Doc 3: "Wilhelm Röntgen"
Wilhelm Röntgen Wilhelm Conrad Röntgen (; ; 27 March 1845 – 10 February 1923) was a German mechanical engineer and physicist, who, on 8 November 1895, produced and detected electromagnetic radiation in a wavelength range known as X-rays or Röntgen rays, an achievement that earned him the first Nobel Prize in Physics in 1901. In honour of his accomplishments, in 2004 the International Union of Pure and Applied Chemistry (IUPAC) named element 111, roentgenium, a radioactive element with multiple unstable isotopes, after him. Born to a German father and a Dutch mother, Röntgen attended high school in Utrecht, Netherlands. In
Doc 4: "Wilhelm Röntgen"
Wilhelm Röntgen Wilhelm Conrad Röntgen (; ; 27 March 1845 – 10 February 1923) was a German mechanical engineer and physicist, who, on 8 November 1895, produced and detected electromagnetic radiation in a wavelength range known as X-rays or Röntgen rays, an achievement that earned him the first Nobel Prize in Physics in 1901. In honour of his accomplishments, in 2004 the International Union of Pure and Applied Chemistry (IUPAC) named element 111, roentgenium, a radioactive element with multiple unstable isotopes, after him. Born to a German father and a Dutch mother, Röntgen attended high school in Utrecht, Netherlands. In
Doc 5: "Nobel Prize"
Wilhelm Röntgen's discovery of X-rays and Philipp Lenard's work on cathode rays. The Academy of Sciences selected Röntgen for the prize. In the last decades of the 19th century, many chemists had made significant contributions. Thus, with the Chemistry Prize, the Academy ""was chiefly faced with merely deciding the order in which these scientists should be awarded the prize"". The Academy received 20 nominations, eleven of them for Jacobus van 't Hoff. Van 't Hoff was awarded the prize for his contributions in chemical thermodynamics. The Swedish Academy chose the poet Sully Prudhomme for the first Nobel Prize in Literature.</information>

<answer>Wilhelm Röntgen</answer>
<|im_start|>system
You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>
<|im_start|>user
Answer the given question. You must conduct reasoning inside <think> and </think> first every time you get new information. After reasoning, if you find you lack some knowledge, you can call a search engine by <search> query </search> and it will return the top searched results between <information> and </information>. You can search as many times as your want. If you find no further external knowledge needed, you can directly provide the answer inside <answer> and </answer>, without detailed illustrations. For example, <answer> Beijing </answer>. Question: when is the next deadpool movie being released?
<|im_end|>
<|im_start|>assistant
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think>
<think
My previous action is invalid. If I want to search, I should put the query between <search> and </search>. If I want to give the final answer, I should put the answer between <answer> and </answer>. Let me try again.
</think>
<search>when is the next deadpool movie being released</search>

<information>Doc 1: "Deadpool (film)"
was written by Reese and Wernick and played in front of ""Logan"". ""Deadpool 2"" was released on May 18, 2018, with Baccarin, T. J. Miller, Uggams, Hildebrand, and Kapičić all returning. Josh Brolin joined them as Cable. The film explores the team X-Force, which includes Deadpool and Cable. In March 2017, Reese said that a future film focused on that group would be separate from ""Deadpool 3"", ""so I think we'll be able to take two paths. [""X-Force""] is where we're launching something bigger, but then [""Deadpool 3"" is] where we're contracting and staying personal and small."" After the acquisition
Doc 2: "Deadpool 2"
is dedicated to her memory. The film's score is the first to receive a parental advisory warning for explicit content, and the soundtrack also includes the original song ""Ashes"" by Céline Dion. ""Deadpool 2"" was released in the United States on May 18, 2018. It has grossed over $738 million worldwide, becoming the seventh highest-grossing film of 2018, as well as the third highest-grossing R-rated film and the third highest-grossing ""X-Men"" film. It received positive reviews from critics, who praised its humor, acting (particularly Reynolds, Brolin, and Beetz's performances), story, and action sequences, with some calling it better than the
Doc 3: "Deadpool 2"
chimichangas, traditionally Deadpool's favorite food, as well as ""Deadpool""-inspired Harder drinks. The campaign also included the chance to win a trip to the film's premiere through Harder, and the Los Angeles pop-up benefitted the nonprofit DTLA Film Festival. ""Deadpool 2"" premiered at Leicester Square in London on May 10, 2018. It was released in the United States on May 18, 2018, having been previously scheduled for release on June 1 of that year. Leitch's initial cut of the film was around two hours and twelve minutes, with ""nips and tucks"" done to it to get the run time down to
Doc 4: "Deadpool 2"
January, the film's release was moved up to May 18, 2018. In February 2018, Terry Crews was revealed to have a role in the film, the character Shatterstar was confirmed to be appearing, and the production returned to Vancouver for six days of reshoots under a new working title, ""Daisy"". Some reports emerged by mid-March claiming that these reshoots were due to poor audience responses during test screenings of the film, and consisted of sweeping changes. However, the film was soon confirmed to be testing better than the original did, up to 98 out of 100 over three different tests
Doc 5: "Deadpool 2"
at the convention moderated by Soni and featuring other cast members. Some of the scenes cut from the theatrical version of the film were debuted at the panel. At the end of September 2018, Fox announced that it would release an untitled ""Deadpool"" film in theaters on December 21. The studio suggested that press and fans ""guess away"" as to what this new film would be, but it was believed to be a re-cut version of ""Deadpool 2"" that would carry a PG-13 rating rather than being R-rated like the initial theatrical release. Reynolds hinted that the version of the</information>

<answer>May 18, 2018</answer>
Traceback (most recent call last):
  File "/ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ssd/chenxi/research/CriticSearch/verl/trainer/main_ppo.py", line 234, in <module>
    main()
  File "/ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
