_wandb:
    value:
        cli_version: 0.22.2
        e:
            5bi8dxkmydr7ae6su5g9ghvvk7kobx0g:
                args:
                    - data.train_files=data/ZeroSearch_dataset/train.parquet
                    - data.val_files=data/ZeroSearch_dataset/test.parquet
                    - data.train_data_num=null
                    - data.val_data_num=null
                    - data.train_batch_size=16
                    - data.val_batch_size=16
                    - data.max_prompt_length=4096
                    - data.max_response_length=500
                    - data.max_start_length=2048
                    - data.max_obs_length=2048
                    - data.shuffle_train_dataloader=True
                    - algorithm.adv_estimator=grpo
                    - actor_rollout_ref.model.path=models/Qwen/Qwen2.5-0.5B-Instruct
                    - actor_rollout_ref.model.enable_gradient_checkpointing=true
                    - actor_rollout_ref.model.use_remove_padding=True
                    - actor_rollout_ref.actor.optim.lr=1e-6
                    - actor_rollout_ref.actor.optim.lr_warmup_steps_ratio=0.95
                    - actor_rollout_ref.actor.use_kl_loss=true
                    - actor_rollout_ref.actor.ppo_mini_batch_size=128
                    - actor_rollout_ref.actor.ppo_micro_batch_size=32
                    - actor_rollout_ref.actor.fsdp_config.param_offload=true
                    - actor_rollout_ref.actor.fsdp_config.grad_offload=true
                    - actor_rollout_ref.actor.fsdp_config.optimizer_offload=true
                    - actor_rollout_ref.rollout.log_prob_micro_batch_size=128
                    - actor_rollout_ref.rollout.tensor_model_parallel_size=1
                    - actor_rollout_ref.rollout.name=vllm
                    - actor_rollout_ref.rollout.gpu_memory_utilization=0.6
                    - actor_rollout_ref.ref.log_prob_micro_batch_size=128
                    - actor_rollout_ref.ref.fsdp_config.param_offload=True
                    - actor_rollout_ref.actor.kl_loss_coef=0.001
                    - actor_rollout_ref.actor.kl_loss_type=low_var_kl
                    - algorithm.no_think_rl=false
                    - actor_rollout_ref.rollout.n_agent=5
                    - actor_rollout_ref.rollout.temperature=1
                    - actor_rollout_ref.actor.state_masking=True
                    - trainer.logger=['wandb']
                    - trainer.val_only=false
                    - trainer.val_before_train=False
                    - trainer.default_hdfs_dir=null
                    - trainer.n_gpus_per_node=2
                    - trainer.nnodes=1
                    - trainer.save_freq=50
                    - trainer.test_freq=300
                    - trainer.project_name=ZeroSearch
                    - trainer.experiment_name=Qwen2.5-0.5B-Instruct_GRPO_simulate_sft_Simulation_LLM_wiki_3B_0_0.5_wiki_turns_5
                    - trainer.total_epochs=10
                    - trainer.total_training_steps=203
                    - trainer.default_hdfs_dir=null
                    - trainer.default_local_dir=verl_checkpoints/Qwen2.5-0.5B-Instruct_GRPO_simulate_sft_Simulation_LLM_wiki_3B_0_0.5_wiki_turns_5
                    - trainer.max_turns=5
                    - trainer.reward_function=f1
                    - trainer.do_search=True
                    - retriever.start_threshold=0
                    - retriever.end_threshold=0.5
                    - retriever.llm_ip=localhost
                    - retriever.search_mode=wiki
                    - retriever.search_engine=wiki
                    - retriever.topk=5
                    - retriever.simulate_llm=models/iic/Simulation_LLM_google_3B
                cpu_count: 32
                cpu_count_logical: 32
                cudaVersion: "12.9"
                disk:
                    /:
                        total: "103240073216"
                        used: "11514712064"
                email: cx9941@gmail.com
                executable: /ssd/chenxi/miniconda3/envs/criticsearch/bin/python
                gpu: NVIDIA GeForce RTX 4090
                gpu_count: 6
                gpu_nvidia:
                    - architecture: Ada
                      cudaCores: 16384
                      memoryTotal: "25757220864"
                      name: NVIDIA GeForce RTX 4090
                      uuid: GPU-367c2750-421b-0e3a-ebac-2dd6f5d8456b
                    - architecture: Ada
                      cudaCores: 16384
                      memoryTotal: "25757220864"
                      name: NVIDIA GeForce RTX 4090
                      uuid: GPU-2c4a9a79-f1ee-cdec-aae0-59921e33b981
                    - architecture: Ada
                      cudaCores: 16384
                      memoryTotal: "25757220864"
                      name: NVIDIA GeForce RTX 4090
                      uuid: GPU-783cb461-769c-319c-9afc-09bf1a8211e2
                    - architecture: Ada
                      cudaCores: 16384
                      memoryTotal: "25757220864"
                      name: NVIDIA GeForce RTX 4090
                      uuid: GPU-1cfd9a2d-21fd-0dde-15c1-b04e7d3e08ba
                    - architecture: Ada
                      cudaCores: 16384
                      memoryTotal: "25757220864"
                      name: NVIDIA GeForce RTX 4090
                      uuid: GPU-79e56c22-507c-f9e8-b544-6a9e46c4fb7a
                    - architecture: Ada
                      cudaCores: 16384
                      memoryTotal: "25757220864"
                      name: NVIDIA GeForce RTX 4090
                      uuid: GPU-2a7dd40f-445a-e658-22e6-4100657a616f
                host: ubuntu
                memory:
                    total: "270254186496"
                os: Linux-6.8.0-63-generic-x86_64-with-glibc2.39
                program: -m verl.trainer.main_ppo
                python: CPython 3.9.24
                root: /ssd/chenxi/research/CriticSearch
                startedAt: "2025-10-16T07:49:26.781572Z"
                writerId: 5bi8dxkmydr7ae6su5g9ghvvk7kobx0g
        m: []
        python_version: 3.9.24
        t:
            "1":
                - 1
                - 11
                - 30
                - 41
                - 49
                - 50
                - 71
                - 95
                - 105
            "2":
                - 1
                - 11
                - 30
                - 41
                - 49
                - 50
                - 71
                - 95
                - 105
            "3":
                - 13
                - 16
            "4": 3.9.24
            "5": 0.22.2
            "6": 4.47.1
            "12": 0.22.2
            "13": linux-x86_64
actor_rollout_ref:
    value:
        actor:
            clip_ratio: 0.2
            entropy_coeff: 0.001
            fsdp_config:
                fsdp_size: -1
                grad_offload: true
                optimizer_offload: true
                param_offload: true
                wrap_policy:
                    min_num_params: 0
            grad_clip: 1
            kl_loss_coef: 0.001
            kl_loss_type: low_var_kl
            optim:
                lr: 1e-06
                lr_warmup_steps_ratio: 0.95
                min_lr_ratio: null
                total_training_steps: 203
                warmup_style: constant
            ppo_epochs: 1
            ppo_max_token_len_per_gpu: 16384
            ppo_micro_batch_size: 32
            ppo_mini_batch_size: 128
            shuffle: false
            state_masking: true
            strategy: fsdp
            ulysses_sequence_parallel_size: 1
            use_dynamic_bsz: false
            use_kl_loss: true
        hybrid_engine: true
        model:
            enable_gradient_checkpointing: true
            external_lib: null
            path: models/Qwen/Qwen2.5-0.5B-Instruct
            use_remove_padding: true
        ref:
            fsdp_config:
                fsdp_size: -1
                param_offload: true
                wrap_policy:
                    min_num_params: 0
            log_prob_max_token_len_per_gpu: 16384
            log_prob_micro_batch_size: 128
            log_prob_use_dynamic_bsz: false
            ulysses_sequence_parallel_size: 1
        rollout:
            do_sample: true
            dtype: bfloat16
            enforce_eager: true
            free_cache_engine: true
            gpu_memory_utilization: 0.6
            ignore_eos: false
            load_format: dummy_dtensor
            log_prob_max_token_len_per_gpu: 16384
            log_prob_micro_batch_size: 128
            log_prob_use_dynamic_bsz: false
            max_num_batched_tokens: 8192
            max_num_seqs: 1024
            "n": 1
            n_agent: 5
            name: vllm
            prompt_length: 4096
            response_length: 500
            temperature: 1
            tensor_model_parallel_size: 1
            top_k: -1
            top_p: 0.95
algorithm:
    value:
        adv_estimator: grpo
        gamma: 1
        kl_ctrl:
            kl_coef: 0.001
            type: fixed
        kl_penalty: kl
        lam: 1
        no_think_rl: false
        state_masking:
            end_state_marker: </information>
            start_state_marker: <information>
critic:
    value:
        cliprange_value: 0.5
        forward_max_token_len_per_gpu: 32768
        forward_micro_batch_size: 64
        grad_clip: 1
        model:
            enable_gradient_checkpointing: false
            external_lib: null
            fsdp_config:
                fsdp_size: -1
                grad_offload: false
                optimizer_offload: false
                param_offload: false
                wrap_policy:
                    min_num_params: 0
            path: ~/models/deepseek-llm-7b-chat
            tokenizer_path: models/Qwen/Qwen2.5-0.5B-Instruct
            use_remove_padding: false
        optim:
            lr: 1e-05
            lr_warmup_steps_ratio: 0
            min_lr_ratio: null
            total_training_steps: 203
            warmup_style: constant
        ppo_epochs: 1
        ppo_max_token_len_per_gpu: 32768
        ppo_micro_batch_size: 64
        ppo_mini_batch_size: 128
        shuffle: false
        strategy: fsdp
        ulysses_sequence_parallel_size: 1
        use_dynamic_bsz: false
data:
    value:
        max_obs_length: 2048
        max_prompt_length: 4096
        max_response_length: 500
        max_start_length: 2048
        prompt_key: prompt
        return_raw_chat: false
        return_raw_input_ids: false
        shuffle_train_dataloader: true
        tokenizer: null
        train_batch_size: 16
        train_data_num: null
        train_files: data/ZeroSearch_dataset/train.parquet
        val_batch_size: 16
        val_data_num: null
        val_files: data/ZeroSearch_dataset/test.parquet
retriever:
    value:
        end_threshold: 0.5
        llm_ip: localhost
        retriever_ip: localhost
        search_engine: wiki
        search_mode: wiki
        simulate_llm: models/iic/Simulation_LLM_google_3B
        start_threshold: 0
        temperature: 0.8
        topk: 5
reward_model:
    value:
        enable: false
        forward_max_token_len_per_gpu: 32768
        max_length: null
        micro_batch_size: 64
        model:
            external_lib: null
            fsdp_config:
                min_num_params: 0
                param_offload: false
            input_tokenizer: models/Qwen/Qwen2.5-0.5B-Instruct
            path: ~/models/FsfairX-LLaMA3-RM-v0.1
            use_remove_padding: false
        strategy: fsdp
        ulysses_sequence_parallel_size: 1
        use_dynamic_bsz: false
trainer:
    value:
        critic_warmup: 0
        default_hdfs_dir: null
        default_local_dir: verl_checkpoints/Qwen2.5-0.5B-Instruct_GRPO_simulate_sft_Simulation_LLM_wiki_3B_0_0.5_wiki_turns_5
        do_critic: true
        do_search: true
        experiment_name: Qwen2.5-0.5B-Instruct_GRPO_simulate_sft_Simulation_LLM_wiki_3B_0_0.5_wiki_turns_5
        logger:
            - wandb
        max_turns: 5
        n_gpus_per_node: 2
        nnodes: 1
        project_name: ZeroSearch
        reward_function: f1
        save_freq: 50
        test_freq: 300
        total_epochs: 10
        total_training_steps: 203
        val_before_train: false
        val_only: false
user:
    value:
        end_threshold: 0.5
        llm_ip: localhost
        simulate_llm: None
        start_threshold: 0.5
        temperature: 0.8
        topk: 5
        user_engine: user
        user_ip: localhost
        user_mode: simulate
