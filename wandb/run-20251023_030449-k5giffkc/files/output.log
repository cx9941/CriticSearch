[34m[1mwandb[0m: Detected [openai] in use.
[34m[1mwandb[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[34m[1mwandb[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/
[36m(pid=2160950)[0m /ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(pid=2160950)[0m No module named 'vllm._version'
[36m(pid=2160950)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(pid=2161127)[0m /ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(pid=2161127)[0m No module named 'vllm._version'
[36m(pid=2161127)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(WorkerDict pid=2160950)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=2160950)[0m   "_name_or_path": "models/Qwen/Qwen2.5-0.5B-Instruct",
[36m(WorkerDict pid=2160950)[0m   "architectures": [
[36m(WorkerDict pid=2160950)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=2160950)[0m   ],
[36m(WorkerDict pid=2160950)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=2160950)[0m   "eos_token_id": 151645,
[36m(WorkerDict pid=2160950)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=2160950)[0m   "hidden_size": 896,
[36m(WorkerDict pid=2160950)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=2160950)[0m   "intermediate_size": 4864,
[36m(WorkerDict pid=2160950)[0m   "max_position_embeddings": 32768,
[36m(WorkerDict pid=2160950)[0m   "max_window_layers": 21,
[36m(WorkerDict pid=2160950)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=2160950)[0m   "num_attention_heads": 14,
[36m(WorkerDict pid=2160950)[0m   "num_hidden_layers": 24,
[36m(WorkerDict pid=2160950)[0m   "num_key_value_heads": 2,
[36m(WorkerDict pid=2160950)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=2160950)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=2160950)[0m   "rope_scaling": null,
[36m(WorkerDict pid=2160950)[0m   "rope_theta": 1000000.0,
[36m(WorkerDict pid=2160950)[0m   "sliding_window": null,
[36m(WorkerDict pid=2160950)[0m   "tie_word_embeddings": true,
[36m(WorkerDict pid=2160950)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=2160950)[0m   "transformers_version": "4.47.1",
[36m(WorkerDict pid=2160950)[0m   "use_cache": true,
[36m(WorkerDict pid=2160950)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=2160950)[0m   "vocab_size": 151936
[36m(WorkerDict pid=2160950)[0m }
[36m(WorkerDict pid=2160950)[0m
[36m(WorkerDict pid=2160950)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(WorkerDict pid=2160950)[0m NCCL version 2.20.5+cuda12.4
[36m(WorkerDict pid=2160950)[0m Qwen2ForCausalLM contains 494.03M parameters
[36m(WorkerDict pid=2160950)[0m wrap_policy: functools.partial(<function _or_policy at 0x7447acd3c5e0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7447acd3c4c0>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
[36m(WorkerDict pid=2160950)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=2160950)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=2160950)[0m   "_name_or_path": "models/Qwen/Qwen2.5-0.5B-Instruct",
[36m(WorkerDict pid=2160950)[0m   "architectures": [
[36m(WorkerDict pid=2160950)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=2160950)[0m   ],
[36m(WorkerDict pid=2160950)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=2160950)[0m   "eos_token_id": 151645,
[36m(WorkerDict pid=2160950)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=2160950)[0m   "hidden_size": 896,
[36m(WorkerDict pid=2160950)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=2160950)[0m   "intermediate_size": 4864,
[36m(WorkerDict pid=2160950)[0m   "max_position_embeddings": 32768,
[36m(WorkerDict pid=2160950)[0m   "max_window_layers": 21,
[36m(WorkerDict pid=2160950)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=2160950)[0m   "num_attention_heads": 14,
[36m(WorkerDict pid=2160950)[0m   "num_hidden_layers": 24,
[36m(WorkerDict pid=2160950)[0m   "num_key_value_heads": 2,
[36m(WorkerDict pid=2160950)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=2160950)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=2160950)[0m   "rope_scaling": null,
[36m(WorkerDict pid=2160950)[0m   "rope_theta": 1000000.0,
[36m(WorkerDict pid=2160950)[0m   "sliding_window": null,
[36m(WorkerDict pid=2160950)[0m   "tie_word_embeddings": true,
[36m(WorkerDict pid=2160950)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=2160950)[0m   "transformers_version": "4.47.1",
[36m(WorkerDict pid=2160950)[0m   "use_cache": true,
[36m(WorkerDict pid=2160950)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=2160950)[0m   "vocab_size": 151936
[36m(WorkerDict pid=2160950)[0m }
[36m(WorkerDict pid=2160950)[0m
[36m(WorkerDict pid=2160950)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[36m(WorkerDict pid=2160950)[0m Qwen2ForCausalLM contains 494.03M parameters
[36m(WorkerDict pid=2160950)[0m Total steps: 203, num_warmup_steps: 192
[36m(WorkerDict pid=2160950)[0m Before building vllm rollout, memory allocated (GB): 0.9205718040466309, memory reserved (GB): 0.958984375
[36m(WorkerDict pid=2160950)[0m WARNING 10-23 03:05:12 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(WorkerDict pid=2161127)[0m wrap_policy: functools.partial(<function _or_policy at 0x7bca5577d5e0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7bca5577d4c0>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(WorkerDict pid=2161127)[0m Actor use_remove_padding=True[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2160950)[0m local rank 0
[36m(WorkerDict pid=2160950)[0m INFO 10-23 03:05:12 selector.py:115] Using XFormers backend.
[36m(WorkerDict pid=2160950)[0m /ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
[36m(WorkerDict pid=2160950)[0m   @torch.library.impl_abstract("xformers_flash::flash_fwd")
[36m(WorkerDict pid=2161127)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(WorkerDict pid=2161127)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[36m(WorkerDict pid=2160950)[0m /ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
[36m(WorkerDict pid=2160950)[0m   @torch.library.impl_abstract("xformers_flash::flash_bwd")
[36m(WorkerDict pid=2161127)[0m Total steps: 203, num_warmup_steps: 192
[36m(WorkerDict pid=2161127)[0m NCCL version 2.20.5+cuda12.4
[36m(WorkerDict pid=2160950)[0m before init cache memory allocated: 1.99052032GB, reserved: 2.057306112GB
[36m(WorkerDict pid=2160950)[0m after init cache memory allocated: 14.57343232GB, reserved: 14.640218112GB
[36m(WorkerDict pid=2161127)[0m WARNING 10-23 03:05:12 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(WorkerDict pid=2161127)[0m local rank 0
[36m(WorkerDict pid=2161127)[0m INFO 10-23 03:05:13 selector.py:115] Using XFormers backend.[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2161127)[0m kwargs: {'n': 1, 'logprobs': 1, 'max_tokens': 500, 'detokenize': False, 'temperature': 1, 'top_k': -1, 'top_p': 0.95, 'ignore_eos': False}
[36m(WorkerDict pid=2161127)[0m /ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=2161127)[0m   warnings.warn(
[36m(WorkerDict pid=2161127)[0m /ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.[32m [repeated 2x across cluster][0m
[36m(WorkerDict pid=2161127)[0m   @torch.library.impl_abstract("xformers_flash::flash_fwd")
[36m(WorkerDict pid=2161127)[0m   @torch.library.impl_abstract("xformers_flash::flash_bwd")
Training:   0%|                                                                                                                                                        | 0/203 [00:00<?, ?it/s]
epoch 0, step 1
[36m(WorkerDict pid=2160950)[0m After building vllm rollout, memory allocated (GB): 12.651166439056396, memory reserved (GB): 13.634765625
[36m(WorkerDict pid=2160950)[0m After building sharding manager, memory allocated (GB): 12.651166439056396, memory reserved (GB): 13.634765625
[2025-10-23 03:05:32,169][httpx][INFO] - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 200 OK"
[2025-10-23 03:05:32,179][httpx][INFO] - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 200 OK"
[2025-10-23 03:05:32,179][httpx][INFO] - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 200 OK"
[2025-10-23 03:05:32,185][httpx][INFO] - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 200 OK"
[2025-10-23 03:05:32,361][httpx][INFO] - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 200 OK"
[2025-10-23 03:05:32,378][httpx][INFO] - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 200 OK"
[2025-10-23 03:05:32,473][httpx][INFO] - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 200 OK"
[2025-10-23 03:05:32,482][httpx][INFO] - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 200 OK"
ACTIVE_TRAJ_NUM: [80, 0]
Interaction Turns Statistics:
Finish at the 1-th turn: 80
Finish at the 2-th turn: 0
Finish at the 3-th turn: 0
Finish at the 4-th turn: 0
Finish at the 5-th turn: 0
Finish at the 6-th turn: 0
[2025-10-23 03:05:44,806][openai._base_client][INFO] - Retrying request to /chat/completions in 0.432101 seconds
[2025-10-23 03:05:45,240][openai._base_client][INFO] - Retrying request to /chat/completions in 0.993137 seconds
[2025-10-23 03:05:46,266][openai._base_client][INFO] - Retrying request to /chat/completions in 0.425711 seconds
[2025-10-23 03:05:46,694][openai._base_client][INFO] - Retrying request to /chat/completions in 0.829885 seconds
[2025-10-23 03:05:47,566][openai._base_client][INFO] - Retrying request to /chat/completions in 0.387642 seconds
[2025-10-23 03:05:47,956][openai._base_client][INFO] - Retrying request to /chat/completions in 0.907639 seconds
[2025-10-23 03:05:48,903][openai._base_client][INFO] - Retrying request to /chat/completions in 0.484018 seconds
[2025-10-23 03:05:49,392][openai._base_client][INFO] - Retrying request to /chat/completions in 0.935235 seconds
[2025-10-23 03:05:50,359][openai._base_client][INFO] - Retrying request to /chat/completions in 0.409569 seconds
[2025-10-23 03:05:50,773][openai._base_client][INFO] - Retrying request to /chat/completions in 0.986158 seconds
[2025-10-23 03:05:51,800][openai._base_client][INFO] - Retrying request to /chat/completions in 0.439084 seconds
[2025-10-23 03:05:52,241][openai._base_client][INFO] - Retrying request to /chat/completions in 0.798037 seconds
[2025-10-23 03:05:53,071][openai._base_client][INFO] - Retrying request to /chat/completions in 0.465941 seconds
[2025-10-23 03:05:53,540][openai._base_client][INFO] - Retrying request to /chat/completions in 0.918725 seconds
[2025-10-23 03:05:54,495][openai._base_client][INFO] - Retrying request to /chat/completions in 0.403779 seconds
[2025-10-23 03:05:54,901][openai._base_client][INFO] - Retrying request to /chat/completions in 0.789955 seconds
[2025-10-23 03:05:55,724][openai._base_client][INFO] - Retrying request to /chat/completions in 0.496063 seconds
[2025-10-23 03:05:56,224][openai._base_client][INFO] - Retrying request to /chat/completions in 0.778549 seconds
[2025-10-23 03:05:57,036][openai._base_client][INFO] - Retrying request to /chat/completions in 0.443271 seconds
[2025-10-23 03:05:57,482][openai._base_client][INFO] - Retrying request to /chat/completions in 0.889700 seconds
[2025-10-23 03:05:58,404][openai._base_client][INFO] - Retrying request to /chat/completions in 0.420188 seconds
[2025-10-23 03:05:58,826][openai._base_client][INFO] - Retrying request to /chat/completions in 0.793232 seconds
[2025-10-23 03:05:59,653][openai._base_client][INFO] - Retrying request to /chat/completions in 0.473954 seconds
[2025-10-23 03:06:00,129][openai._base_client][INFO] - Retrying request to /chat/completions in 0.948061 seconds
[2025-10-23 03:06:01,126][openai._base_client][INFO] - Retrying request to /chat/completions in 0.499572 seconds
[2025-10-23 03:06:01,628][openai._base_client][INFO] - Retrying request to /chat/completions in 0.875771 seconds
[2025-10-23 03:06:02,536][openai._base_client][INFO] - Retrying request to /chat/completions in 0.478889 seconds
[2025-10-23 03:06:03,017][openai._base_client][INFO] - Retrying request to /chat/completions in 0.781205 seconds
[2025-10-23 03:06:03,831][openai._base_client][INFO] - Retrying request to /chat/completions in 0.465559 seconds
[2025-10-23 03:06:04,298][openai._base_client][INFO] - Retrying request to /chat/completions in 0.930455 seconds
[2025-10-23 03:06:05,261][openai._base_client][INFO] - Retrying request to /chat/completions in 0.403425 seconds
[2025-10-23 03:06:05,670][openai._base_client][INFO] - Retrying request to /chat/completions in 0.856293 seconds
[2025-10-23 03:06:06,568][openai._base_client][INFO] - Retrying request to /chat/completions in 0.393234 seconds
[2025-10-23 03:06:06,963][openai._base_client][INFO] - Retrying request to /chat/completions in 0.853797 seconds
[2025-10-23 03:06:07,849][openai._base_client][INFO] - Retrying request to /chat/completions in 0.498320 seconds
[2025-10-23 03:06:08,349][openai._base_client][INFO] - Retrying request to /chat/completions in 0.838554 seconds
[2025-10-23 03:06:09,220][openai._base_client][INFO] - Retrying request to /chat/completions in 0.444408 seconds
[2025-10-23 03:06:09,666][openai._base_client][INFO] - Retrying request to /chat/completions in 0.774553 seconds
[2025-10-23 03:06:10,477][openai._base_client][INFO] - Retrying request to /chat/completions in 0.495420 seconds
[2025-10-23 03:06:10,974][openai._base_client][INFO] - Retrying request to /chat/completions in 0.803909 seconds
[2025-10-23 03:06:11,814][openai._base_client][INFO] - Retrying request to /chat/completions in 0.392410 seconds
[2025-10-23 03:06:12,208][openai._base_client][INFO] - Retrying request to /chat/completions in 0.846377 seconds
[2025-10-23 03:06:13,087][openai._base_client][INFO] - Retrying request to /chat/completions in 0.485854 seconds
[2025-10-23 03:06:13,585][openai._base_client][INFO] - Retrying request to /chat/completions in 0.828761 seconds
[2025-10-23 03:06:14,446][openai._base_client][INFO] - Retrying request to /chat/completions in 0.419662 seconds
[2025-10-23 03:06:14,867][openai._base_client][INFO] - Retrying request to /chat/completions in 0.920545 seconds
[2025-10-23 03:06:15,820][openai._base_client][INFO] - Retrying request to /chat/completions in 0.383548 seconds
[2025-10-23 03:06:16,206][openai._base_client][INFO] - Retrying request to /chat/completions in 0.820058 seconds
[2025-10-23 03:06:17,058][openai._base_client][INFO] - Retrying request to /chat/completions in 0.481745 seconds
[2025-10-23 03:06:17,542][openai._base_client][INFO] - Retrying request to /chat/completions in 0.945979 seconds
[2025-10-23 03:06:18,520][openai._base_client][INFO] - Retrying request to /chat/completions in 0.395747 seconds
[2025-10-23 03:06:18,918][openai._base_client][INFO] - Retrying request to /chat/completions in 0.957363 seconds
[2025-10-23 03:06:19,914][openai._base_client][INFO] - Retrying request to /chat/completions in 0.392868 seconds
[2025-10-23 03:06:20,309][openai._base_client][INFO] - Retrying request to /chat/completions in 0.983150 seconds
[2025-10-23 03:06:21,324][openai._base_client][INFO] - Retrying request to /chat/completions in 0.430300 seconds
[2025-10-23 03:06:21,757][openai._base_client][INFO] - Retrying request to /chat/completions in 0.855809 seconds
[2025-10-23 03:06:22,645][openai._base_client][INFO] - Retrying request to /chat/completions in 0.383734 seconds
[2025-10-23 03:06:23,030][openai._base_client][INFO] - Retrying request to /chat/completions in 0.872415 seconds
[2025-10-23 03:06:23,935][openai._base_client][INFO] - Retrying request to /chat/completions in 0.499684 seconds
[2025-10-23 03:06:24,437][openai._base_client][INFO] - Retrying request to /chat/completions in 0.855598 seconds
[2025-10-23 03:06:25,326][openai._base_client][INFO] - Retrying request to /chat/completions in 0.387615 seconds
[2025-10-23 03:06:25,724][openai._base_client][INFO] - Retrying request to /chat/completions in 0.763077 seconds
[2025-10-23 03:06:26,520][openai._base_client][INFO] - Retrying request to /chat/completions in 0.479826 seconds
[2025-10-23 03:06:27,003][openai._base_client][INFO] - Retrying request to /chat/completions in 0.872899 seconds
[2025-10-23 03:06:27,916][openai._base_client][INFO] - Retrying request to /chat/completions in 0.427816 seconds
[2025-10-23 03:06:28,346][openai._base_client][INFO] - Retrying request to /chat/completions in 0.991056 seconds
[2025-10-23 03:06:29,369][openai._base_client][INFO] - Retrying request to /chat/completions in 0.431848 seconds
[2025-10-23 03:06:29,803][openai._base_client][INFO] - Retrying request to /chat/completions in 0.957834 seconds
[2025-10-23 03:06:30,800][openai._base_client][INFO] - Retrying request to /chat/completions in 0.385367 seconds
[2025-10-23 03:06:31,187][openai._base_client][INFO] - Retrying request to /chat/completions in 0.956157 seconds
[2025-10-23 03:06:32,176][openai._base_client][INFO] - Retrying request to /chat/completions in 0.498189 seconds
[2025-10-23 03:06:32,678][openai._base_client][INFO] - Retrying request to /chat/completions in 0.793457 seconds
[2025-10-23 03:06:33,505][openai._base_client][INFO] - Retrying request to /chat/completions in 0.468120 seconds
[2025-10-23 03:06:33,975][openai._base_client][INFO] - Retrying request to /chat/completions in 0.777931 seconds
[2025-10-23 03:06:34,787][openai._base_client][INFO] - Retrying request to /chat/completions in 0.480498 seconds
[2025-10-23 03:06:35,274][openai._base_client][INFO] - Retrying request to /chat/completions in 0.893906 seconds
[2025-10-23 03:06:36,200][openai._base_client][INFO] - Retrying request to /chat/completions in 0.481357 seconds
[2025-10-23 03:06:36,691][openai._base_client][INFO] - Retrying request to /chat/completions in 0.853197 seconds
[2025-10-23 03:06:37,587][openai._base_client][INFO] - Retrying request to /chat/completions in 0.384386 seconds
[2025-10-23 03:06:37,973][openai._base_client][INFO] - Retrying request to /chat/completions in 0.822330 seconds
[2025-10-23 03:06:38,828][openai._base_client][INFO] - Retrying request to /chat/completions in 0.444596 seconds
[2025-10-23 03:06:39,275][openai._base_client][INFO] - Retrying request to /chat/completions in 0.785375 seconds
[2025-10-23 03:06:40,092][openai._base_client][INFO] - Retrying request to /chat/completions in 0.495259 seconds
[2025-10-23 03:06:40,590][openai._base_client][INFO] - Retrying request to /chat/completions in 0.765153 seconds
[2025-10-23 03:06:41,387][openai._base_client][INFO] - Retrying request to /chat/completions in 0.414639 seconds
[2025-10-23 03:06:41,804][openai._base_client][INFO] - Retrying request to /chat/completions in 0.856046 seconds
[2025-10-23 03:06:42,699][openai._base_client][INFO] - Retrying request to /chat/completions in 0.439426 seconds
[2025-10-23 03:06:43,140][openai._base_client][INFO] - Retrying request to /chat/completions in 0.883772 seconds
[2025-10-23 03:06:44,056][openai._base_client][INFO] - Retrying request to /chat/completions in 0.479691 seconds
[2025-10-23 03:06:44,538][openai._base_client][INFO] - Retrying request to /chat/completions in 0.971833 seconds
[2025-10-23 03:06:45,542][openai._base_client][INFO] - Retrying request to /chat/completions in 0.386626 seconds
[2025-10-23 03:06:45,931][openai._base_client][INFO] - Retrying request to /chat/completions in 0.806943 seconds
[2025-10-23 03:06:46,771][openai._base_client][INFO] - Retrying request to /chat/completions in 0.475737 seconds
[2025-10-23 03:06:47,248][openai._base_client][INFO] - Retrying request to /chat/completions in 0.848493 seconds
Traceback (most recent call last):
  File "/ssd/chenxi/research/CriticSearch/llm_agent/generation.py", line 593, in batch_search
    for future in as_completed(futures):
  File "/ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/concurrent/futures/_base.py", line 245, in as_completed
    waiter.event.wait(wait_timeout)
  File "/ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/threading.py", line 581, in wait
    signaled = self._cond.wait(timeout)
  File "/ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/threading.py", line 312, in wait
    waiter.acquire()
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ssd/chenxi/research/CriticSearch/verl/trainer/main_ppo.py", line 233, in <module>
    main()
  File "/ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/ssd/chenxi/research/CriticSearch/verl/trainer/main_ppo.py", line 156, in main
    main_task(config)
  File "/ssd/chenxi/research/CriticSearch/verl/trainer/main_ppo.py", line 229, in main_task
    trainer.fit()
  File "/ssd/chenxi/research/CriticSearch/verl/trainer/ppo/ray_trainer.py", line 860, in fit
    final_gen_batch_output, trajectory_turns = generation_manager.run_llm_loop(
  File "/ssd/chenxi/research/CriticSearch/llm_agent/generation.py", line 371, in run_llm_loop
    next_obs, dones, valid_action, is_search = self.execute_predictions(
  File "/ssd/chenxi/research/CriticSearch/llm_agent/generation.py", line 505, in execute_predictions
    search_results = self.batch_search(search_queries, problem, ground_truth, search_mode, gt_threshold)
  File "/ssd/chenxi/research/CriticSearch/llm_agent/generation.py", line 598, in batch_search
    continue
  File "/ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/concurrent/futures/_base.py", line 637, in __exit__
    self.shutdown(wait=True)
  File "/ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/concurrent/futures/thread.py", line 235, in shutdown
    t.join()
  File "/ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/threading.py", line 1060, in join
    self._wait_for_tstate_lock()
  File "/ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/threading.py", line 1080, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
KeyboardInterrupt
Traceback (most recent call last):
  File "/ssd/chenxi/research/CriticSearch/llm_agent/generation.py", line 593, in batch_search
    for future in as_completed(futures):
  File "/ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/concurrent/futures/_base.py", line 245, in as_completed
    waiter.event.wait(wait_timeout)
  File "/ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/threading.py", line 581, in wait
    signaled = self._cond.wait(timeout)
  File "/ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/threading.py", line 312, in wait
    waiter.acquire()
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ssd/chenxi/research/CriticSearch/verl/trainer/main_ppo.py", line 233, in <module>
    main()
  File "/ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/ssd/chenxi/research/CriticSearch/verl/trainer/main_ppo.py", line 156, in main
    main_task(config)
  File "/ssd/chenxi/research/CriticSearch/verl/trainer/main_ppo.py", line 229, in main_task
    trainer.fit()
  File "/ssd/chenxi/research/CriticSearch/verl/trainer/ppo/ray_trainer.py", line 860, in fit
    final_gen_batch_output, trajectory_turns = generation_manager.run_llm_loop(
  File "/ssd/chenxi/research/CriticSearch/llm_agent/generation.py", line 371, in run_llm_loop
    next_obs, dones, valid_action, is_search = self.execute_predictions(
  File "/ssd/chenxi/research/CriticSearch/llm_agent/generation.py", line 505, in execute_predictions
    search_results = self.batch_search(search_queries, problem, ground_truth, search_mode, gt_threshold)
  File "/ssd/chenxi/research/CriticSearch/llm_agent/generation.py", line 598, in batch_search
    continue
  File "/ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/concurrent/futures/_base.py", line 637, in __exit__
    self.shutdown(wait=True)
  File "/ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/concurrent/futures/thread.py", line 235, in shutdown
    t.join()
  File "/ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/threading.py", line 1060, in join
    self._wait_for_tstate_lock()
  File "/ssd/chenxi/miniconda3/envs/criticsearch/lib/python3.9/threading.py", line 1080, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
KeyboardInterrupt
